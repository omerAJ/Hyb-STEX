{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (data, _) in enumerate(train_loader):\n",
    "        data = data[:, 0, :, :].squeeze(1)\n",
    "        B, N, D = data.size()\n",
    "        data = data.view(B, args[\"rows\"], args[\"cols\"], D).to(device)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(data[1, :, :, 1].cpu().numpy(), cmap='viridis', aspect='auto')\n",
    "        plt.colorbar(label='no. of cars')\n",
    "        plt.title('Heatmap showing traffic in the city')\n",
    "        plt.xlabel('cols')\n",
    "        plt.ylabel('rows')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from model.vision_transformer_utils import (\n",
    "    WarmupCosineSchedule,\n",
    "    CosineWDSchedule)\n",
    "from model.vision_transformer_utils import trunc_normal_, init_opt, apply_masks_targets\n",
    "from model.ijepa_utils import AverageMeter\n",
    "from model.vision_transformer import VisionTransformer, VisionTransformerPredictor\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    device,\n",
    "    r_path,\n",
    "    encoder,\n",
    "    predictor,\n",
    "    target_encoder,\n",
    "    opt,\n",
    "    scaler,\n",
    "):\n",
    "    try:\n",
    "        checkpoint = torch.load(r_path, map_location=torch.device('cpu'))\n",
    "        epoch = checkpoint['epoch']\n",
    "\n",
    "        # -- loading encoder\n",
    "        pretrained_dict = checkpoint['encoder']\n",
    "        msg = encoder.load_state_dict(pretrained_dict)\n",
    "        logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
    "\n",
    "        # -- loading predictor\n",
    "        pretrained_dict = checkpoint['predictor']\n",
    "        msg = predictor.load_state_dict(pretrained_dict)\n",
    "        logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
    "\n",
    "        # -- loading target_encoder\n",
    "        if target_encoder is not None:\n",
    "            print(list(checkpoint.keys()))\n",
    "            pretrained_dict = checkpoint['target_encoder']\n",
    "            msg = target_encoder.load_state_dict(pretrained_dict)\n",
    "            logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
    "\n",
    "        # -- loading optimizer\n",
    "        if opt is not None:\n",
    "            opt.load_state_dict(checkpoint['opt'])\n",
    "        if scaler is not None:\n",
    "            scaler.load_state_dict(checkpoint['scaler'])\n",
    "        logger.info(f'loaded optimizers from epoch {epoch}')\n",
    "        logger.info(f'read-path: {r_path}')\n",
    "        del checkpoint\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.info(f'Encountered exception when loading checkpoint {e}')\n",
    "        epoch = 0\n",
    "\n",
    "    return encoder, predictor, target_encoder, opt, scaler, epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data['x_train'].shape:  (1912, 35, 200, 2) (1912, 1, 200, 2)\n",
      "INFO:root:Using AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\omer\\ST-SSL\\lib\\dataloader.py:63: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:85.)\n",
      "  X, Y = TensorFloat(X), TensorFloat(Y)\n",
      "c:\\Users\\CITY3\\.conda\\envs\\i-jepaVENV\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "load_model=True\n",
    "import time\n",
    "import yaml\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "from lib.dataloader import get_dataloader\n",
    "\n",
    "\n",
    "# start_epoch = args.start_epoch\n",
    "# num_epochs = args.num_epochs\n",
    "\n",
    "args = {\"data_dir\": r\"data\", \"dataset\": \"NYCTaxi\", \"batch_size\": 32, \"test_batch_size\": 32, \"rows\": 20, \"cols\": 10, \"num_epochs\": 100}\n",
    "dataloader = get_dataloader(\n",
    "    data_dir=args['data_dir'], \n",
    "    dataset=args['dataset'], \n",
    "    batch_size=args['batch_size'], \n",
    "    test_batch_size=args['test_batch_size'],\n",
    "    scalar_type='Standard'\n",
    ")\n",
    "train_loader = dataloader['train']\n",
    "test_loader = dataloader['test']\n",
    "val_loader = dataloader['val']\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = VisionTransformer(\n",
    "        img_size=(args[\"rows\"], args[\"cols\"]),\n",
    "        patch_size=1,\n",
    "        in_chans=2,\n",
    "        embed_dim=64,\n",
    "        predictor_embed_dim=None,\n",
    "        depth=1,\n",
    "        predictor_depth=None,\n",
    "        num_heads=1,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.2,\n",
    "        drop_path_rate=0.1,\n",
    "        norm_layer=torch.nn.LayerNorm,\n",
    "        init_std=0.02\n",
    "    )\n",
    "predictor = VisionTransformerPredictor(\n",
    "    img_size=(args[\"rows\"], args[\"cols\"]),\n",
    "    embed_dim=64,\n",
    "    predictor_embed_dim=64//2,\n",
    "    depth=1,\n",
    "    num_heads=1,\n",
    "    mlp_ratio=4,\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.2,\n",
    "    drop_path_rate=0.1,\n",
    "    norm_layer=torch.nn.LayerNorm,\n",
    "    init_std=0.02\n",
    ")\n",
    "\n",
    "import copy\n",
    "target_encoder = copy.deepcopy(encoder)\n",
    "\n",
    "wd = 0.04\n",
    "final_wd = 0.4\n",
    "start_lr = 0.0002\n",
    "final_lr = 1.0e-06\n",
    "lr = 0.001\n",
    "ipe = len(train_loader)\n",
    "warmup = 40\n",
    "ipe_scale = 1.0\n",
    "use_bfloat16 = True\n",
    "optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
    "        encoder=encoder,\n",
    "        predictor=predictor,\n",
    "        wd=wd,\n",
    "        final_wd=final_wd,\n",
    "        start_lr=start_lr,\n",
    "        ref_lr=lr,\n",
    "        final_lr=final_lr,\n",
    "        iterations_per_epoch=ipe,\n",
    "        warmup=warmup,\n",
    "        num_epochs=args[\"num_epochs\"],\n",
    "        ipe_scale=ipe_scale,\n",
    "        use_bfloat16=use_bfloat16)\n",
    "\n",
    "ema = [0.996, 1.0]\n",
    "ipe = len(train_loader)\n",
    "ipe_scale = 1.0\n",
    "momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*args[\"num_epochs\"]*ipe_scale)\n",
    "                        for i in range(int(ipe*args[\"num_epochs\"]*ipe_scale)+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = r\"D:\\omer\\ST-SSL\\logs\\singleBLK\\jepa-ep90.pth.tar\"\n",
    "\n",
    "if load_model:\n",
    "    encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
    "        device=device,\n",
    "        r_path=load_path,\n",
    "        encoder=encoder,\n",
    "        predictor=predictor,\n",
    "        target_encoder=target_encoder,\n",
    "        opt=optimizer,\n",
    "        scaler=scaler)\n",
    "    for _ in range(start_epoch*ipe):\n",
    "        scheduler.step()\n",
    "        wd_scheduler.step()\n",
    "        next(momentum_scheduler)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMasks(data):\n",
    "    \"\"\"\n",
    "    :param data: tensor of shape [B, R, C, D]\n",
    "    :returns: (data, 1x masks_enc, 4x masks_pred)\n",
    "    \"\"\"\n",
    "    B, R, C, D = data.size()\n",
    "\n",
    "    # Initialize masks\n",
    "    masks_enc = torch.zeros(B, R, C, dtype=torch.uint8)\n",
    "    masks_pred = torch.zeros(4, B, R, C, dtype=torch.uint8)\n",
    "\n",
    "    ## select deltas before sample loop. \n",
    "    ## grid_size: 20x10\n",
    "    delta_h_ctxt = torch.randint(6, 10, (1,))        ## low (inclusive), high (exclusive)\n",
    "    delta_w_ctxt = torch.randint(4, 6, (1,))  \n",
    "    delta_h_trgt = torch.randint(4, 8, (1,))  \n",
    "    delta_w_trgt = torch.randint(2, 4, (1,))  \n",
    "    \n",
    "    for b in range(B):\n",
    "        # Select h1, w1 for the encoding mask with constraints\n",
    "        h1 = torch.randint(0, R, (1,))\n",
    "        if h1 + delta_h_ctxt > R:\n",
    "            h1 = R - delta_h_ctxt\n",
    "        w1 = torch.randint(0, C, (1,))\n",
    "        if w1 + delta_w_ctxt > C:\n",
    "            w1 = C - delta_w_ctxt\n",
    "        \n",
    "        # Set the encoding mask\n",
    "        masks_enc[b, h1:h1+delta_h_ctxt, w1:w1+delta_w_ctxt] = 1\n",
    "\n",
    "        # Generate four prediction masks (can be overlapping with each other, but not with context mask)\n",
    "        for i in range(4):\n",
    "            n=0\n",
    "            while True:\n",
    "                if n>2000:\n",
    "                    print(f\"cant find valid mask n={n}, stuck...\")\n",
    "                # Smaller random sizes for prediction masks\n",
    "                h1 = torch.randint(0, R, (1,))\n",
    "                if h1 + delta_h_trgt > R:\n",
    "                    h1 = R - delta_h_trgt\n",
    "                w1 = torch.randint(0, C, (1,))\n",
    "                if w1 + delta_w_trgt > C:\n",
    "                    w1 = C - delta_w_trgt\n",
    "\n",
    "                # Create a temporary mask to check overlap\n",
    "                temp_mask = torch.zeros(R, C, dtype=torch.bool)\n",
    "                temp_mask[h1:h1+delta_h_trgt, w1:w1+delta_w_trgt] = 1\n",
    "\n",
    "                # Check if it overlaps with the encoding mask\n",
    "                if torch.any(masks_enc[b] & temp_mask):\n",
    "                    n+=1\n",
    "                    continue  # Overlaps, try again\n",
    "\n",
    "                # No overlap, set this mask\n",
    "                masks_pred[i, b] = temp_mask\n",
    "                break\n",
    "\n",
    "    return (data, masks_enc, masks_pred.transpose(0, 1))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# def evaluate_model(data_loader, encoder, predictor, target_encoder, device):\n",
    "#     \"\"\"\n",
    "#     Perform evaluation of the model on the given data loader.\n",
    "    \n",
    "#     Args:\n",
    "#         data_loader (DataLoader): The data loader for the dataset to evaluate.\n",
    "#         encoder (nn.Module): The encoder model.\n",
    "#         predictor (nn.Module): The predictor model.\n",
    "#         target_encoder (nn.Module): The target encoder model.\n",
    "#         device (str): The device to run the evaluation on, e.g., 'cuda' or 'cpu'.\n",
    "    \n",
    "#     Returns:\n",
    "#         float: The average loss over the evaluation dataset.\n",
    "#     \"\"\"\n",
    "#     encoder.eval().to(device)\n",
    "#     predictor.eval().to(device)\n",
    "#     target_encoder.eval().to(device)\n",
    "#     loss_meter = AverageMeter()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for data, _ in data_loader:\n",
    "#             data = data[:, 0, :, :].squeeze(1)  # Simplify data handling for evaluation\n",
    "#             B, N, D = data.size()\n",
    "#             data = data.view(B, args[\"rows\"], args[\"cols\"], D).to(device)\n",
    "#             imgs, masks_enc, masks_pred = generateMasks(data)\n",
    "#             imgs = imgs.permute(0, 3, 1, 2)  # [B, D, R, C]\n",
    "#             masks_pred = masks_pred.flatten(2)  # [B, 4, R*C]\n",
    "#             masks_enc = masks_enc.flatten(1).unsqueeze(1)  # [B, 1, R*C]\n",
    "            \n",
    "#             \"\"\" see whole image as context \"\"\"\n",
    "#             masks_enc=torch.ones_like(masks_enc)\n",
    "\n",
    "\n",
    "#             # Forward pass through the models\n",
    "#             def forward_target():    ## mask target tokens after encoding\n",
    "#                 with torch.no_grad():  #sg\n",
    "#                     h = target_encoder(imgs, masks=None)  ## VisionTransformer  masks_enc=None\n",
    "#                     h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
    "#                     B = len(h)\n",
    "#                     h = apply_masks_targets(h, masks_pred)\n",
    "#                     return h\n",
    "\n",
    "#             def forward_context():    ## mask context tokens before encoding\n",
    "                \n",
    "#                 z = encoder(imgs, masks=masks_enc)  ## VisionTransformer\n",
    "#                 \"\"\"input to predictor is z: [32, 45, 256]\"\"\"\n",
    "#                 z = predictor(z, masks_enc, masks_pred)   ## VisionTransformerPredictor\n",
    "#                 return z\n",
    "#             h = forward_target()  # Target encoder forward pass\n",
    "#             z = forward_context()  # Context encoder forward pass\n",
    "            \n",
    "#             # Compute loss\n",
    "#             loss = F.smooth_l1_loss(z, h)\n",
    "#             loss_meter.update(loss.item(), B)\n",
    "    \n",
    "#     return loss_meter.avg\n",
    "\n",
    "# loss = evaluate_model(val_loader, encoder, predictor, target_encoder, 'cuda')\n",
    "# print(\"Validation Loss: \", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = VisionTransformer(\n",
    "        img_size=(args[\"rows\"], args[\"cols\"]),\n",
    "        patch_size=1,\n",
    "        in_chans=2,\n",
    "        embed_dim=256,\n",
    "        predictor_embed_dim=None,\n",
    "        depth=6,\n",
    "        predictor_depth=None,\n",
    "        num_heads=1,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.2,\n",
    "        drop_path_rate=0.1,\n",
    "        norm_layer=torch.nn.LayerNorm,\n",
    "        init_std=0.02\n",
    "    )\n",
    "predictor = VisionTransformerPredictor(\n",
    "    img_size=(args[\"rows\"], args[\"cols\"]),\n",
    "    embed_dim=256,\n",
    "    predictor_embed_dim=256//2,\n",
    "    depth=6,\n",
    "    num_heads=1,\n",
    "    mlp_ratio=4,\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.2,\n",
    "    drop_path_rate=0.1,\n",
    "    norm_layer=torch.nn.LayerNorm,\n",
    "    init_std=0.02\n",
    ")\n",
    "\n",
    "import copy\n",
    "target_encoder = copy.deepcopy(encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = VisionTransformer(\n",
    "        img_size=(args[\"rows\"], args[\"cols\"]),\n",
    "        patch_size=1,\n",
    "        in_chans=2,\n",
    "        embed_dim=256,\n",
    "        predictor_embed_dim=None,\n",
    "        depth=6,\n",
    "        predictor_depth=None,\n",
    "        num_heads=1,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.2,\n",
    "        drop_path_rate=0.1,\n",
    "        norm_layer=torch.nn.LayerNorm,\n",
    "        init_std=0.02\n",
    "    )\n",
    "predictor = VisionTransformerPredictor(\n",
    "    img_size=(args[\"rows\"], args[\"cols\"]),\n",
    "    embed_dim=256,\n",
    "    predictor_embed_dim=256//2,\n",
    "    depth=6,\n",
    "    num_heads=1,\n",
    "    mlp_ratio=4,\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.2,\n",
    "    drop_path_rate=0.1,\n",
    "    norm_layer=torch.nn.LayerNorm,\n",
    "    init_std=0.02\n",
    ")\n",
    "\n",
    "import copy\n",
    "target_encoder = copy.deepcopy(encoder)\n",
    "\n",
    "\n",
    "load_path = r\"D:\\omer\\ST-SSL\\logs\\pretrain_logs\\jepa-ep50.pth.tar\"\n",
    "\n",
    "if load_model:\n",
    "    encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
    "        device=device,\n",
    "        r_path=load_path,\n",
    "        encoder=encoder,\n",
    "        predictor=predictor,\n",
    "        target_encoder=target_encoder,\n",
    "        opt=optimizer,\n",
    "        scaler=scaler)\n",
    "    for _ in range(start_epoch*ipe):\n",
    "        scheduler.step()\n",
    "        wd_scheduler.step()\n",
    "\n",
    "        # next(momentum_scheduler)\n",
    "## just get the attention maps from the context_encoder (input whole image)\n",
    "encoder.eval().to(device)\n",
    "predictor.eval().to(device)\n",
    "target_encoder.eval().to(device)\n",
    "with torch.no_grad():\n",
    "    for i, (data, _) in enumerate(val_loader):\n",
    "        data = data[:, 0, :, :].squeeze(1)\n",
    "        B, N, D = data.size()\n",
    "        data = data.view(B, args[\"rows\"], args[\"cols\"], D).to(device)\n",
    "        imgs, masks_enc, masks_pred = generateMasks(data)\n",
    "        imgs = imgs.permute(0, 3, 1, 2)  # [B, D, R, C]\n",
    "        masks_pred = masks_pred.flatten(2)  # [B, 4, R*C]\n",
    "        masks_enc = masks_enc.flatten(1).unsqueeze(1)  # [B, 1, R*C]\n",
    "        \n",
    "        \"\"\" see whole image as context \"\"\"\n",
    "        masks_enc=torch.ones_like(masks_enc)\n",
    "        # masks_enc=torch.zeros_like(masks_enc)\n",
    "        # masks_enc[:, :, 0:2] = 1\n",
    "        def forward_context():    ## mask context tokens before encoding\n",
    "            z, attn_list = encoder(imgs, masks=masks_enc)  ## VisionTransformer\n",
    "            return z, attn_list\n",
    "        \n",
    "        z, attn_list_1 = forward_context()  # Context encoder forward pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(attn_list))\n",
    "a=0\n",
    "print(attn_list_1[0].shape)\n",
    "for i in range(7):\n",
    "    if i == 6:\n",
    "        attn1 = average_attention[a:a+20,:200]\n",
    "    else:\n",
    "        attn1 = attn_list_1[i][0].squeeze(0).softmax(-1).cpu().numpy()[a:a+20,:200]\n",
    "    print(attn1.shape)\n",
    "\n",
    "    # avg_attn = torch.tensor(average_attention).softmax(-1).numpy()[100:120,:200]\n",
    "    # avg_attn = average_attention[0:20,:200]\n",
    "    import numpy as np\n",
    "    import plotly.figure_factory as ff\n",
    "\n",
    "    # Create a heatmap using Plotly, attempting again\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        z=attn1,\n",
    "        colorscale='Magma',\n",
    "        showscale=True\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title='Interactive Attention Matrix Heatmap',\n",
    "        width=900,\n",
    "        height=900,\n",
    "        xaxis=dict(ticks='', side='top'),\n",
    "        yaxis=dict(ticks='')\n",
    "    )\n",
    "    print(\"\\n\\ni: \", i)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1 = attn_list[0][0].squeeze(0).softmax(-1).cpu().numpy().transpose()[:,0:300]\n",
    "print(attn1.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(attn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(attn_list))\n",
    "a=80\n",
    "print(attn_list[0].shape)\n",
    "for i in range(6):\n",
    "    if i == 6:\n",
    "        attn1 = average_attention[a:a+20,:200]\n",
    "    else:\n",
    "        attn1 = attn_list[i][0].squeeze(0).softmax(-1).cpu().numpy()[a:a+20,:200]\n",
    "    print(attn1.shape)\n",
    "\n",
    "    # avg_attn = torch.tensor(average_attention).softmax(-1).numpy()[100:120,:200]\n",
    "    # avg_attn = average_attention[0:20,:200]\n",
    "    import numpy as np\n",
    "    import plotly.figure_factory as ff\n",
    "\n",
    "    # Create a heatmap using Plotly, attempting again\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        z=attn1,\n",
    "        colorscale='Magma',\n",
    "        showscale=True\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title='Interactive Attention Matrix Heatmap',\n",
    "        width=900,\n",
    "        height=900,\n",
    "        xaxis=dict(ticks='', side='top'),\n",
    "        yaxis=dict(ticks='')\n",
    "    )\n",
    "    print(\"\\n\\ni: \", i)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adj(path, l):\n",
    "    load_model=True\n",
    "    import time\n",
    "    import yaml\n",
    "    sys.path.append('.')\n",
    "    sys.path.append('..')\n",
    "    from lib.dataloader import get_dataloader\n",
    "    args = {\"data_dir\": r\"data\", \"dataset\": \"NYCTaxi\", \"batch_size\": 32, \"test_batch_size\": 32, \"rows\": 20, \"cols\": 10, \"num_epochs\": 100}\n",
    "    dataloader = get_dataloader(\n",
    "        data_dir=args['data_dir'], \n",
    "        dataset=args['dataset'], \n",
    "        batch_size=args['batch_size'], \n",
    "        test_batch_size=args['test_batch_size'],\n",
    "        scalar_type='Standard'\n",
    "    )\n",
    "    train_loader = dataloader['train']\n",
    "    test_loader = dataloader['test']\n",
    "    val_loader = dataloader['val']\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder = VisionTransformer(\n",
    "            img_size=(args[\"rows\"], args[\"cols\"]),\n",
    "            patch_size=1,\n",
    "            in_chans=2,\n",
    "            embed_dim=64,\n",
    "            predictor_embed_dim=None,\n",
    "            depth=1,\n",
    "            predictor_depth=None,\n",
    "            num_heads=1,\n",
    "            mlp_ratio=4,\n",
    "            qkv_bias=True,\n",
    "            qk_scale=None,\n",
    "            drop_rate=0.0,\n",
    "            attn_drop_rate=0.2,\n",
    "            drop_path_rate=0.1,\n",
    "            norm_layer=torch.nn.LayerNorm,\n",
    "            init_std=0.02\n",
    "        )\n",
    "    predictor = VisionTransformerPredictor(\n",
    "        img_size=(args[\"rows\"], args[\"cols\"]),\n",
    "        embed_dim=64,\n",
    "        predictor_embed_dim=64//2,\n",
    "        depth=1,\n",
    "        num_heads=1,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.2,\n",
    "        drop_path_rate=0.1,\n",
    "        norm_layer=torch.nn.LayerNorm,\n",
    "        init_std=0.02\n",
    "    )\n",
    "\n",
    "    import copy\n",
    "    target_encoder = copy.deepcopy(encoder)\n",
    "    encoder.eval().to(device)\n",
    "    predictor.eval().to(device)\n",
    "    target_encoder.eval().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    wd = 0.04\n",
    "    final_wd = 0.4\n",
    "    start_lr = 0.0002\n",
    "    final_lr = 1.0e-06\n",
    "    lr = 0.001\n",
    "    ipe = len(train_loader)\n",
    "    warmup = 40\n",
    "    ipe_scale = 1.0\n",
    "    use_bfloat16 = True\n",
    "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
    "            encoder=encoder,\n",
    "            predictor=predictor,\n",
    "            wd=wd,\n",
    "            final_wd=final_wd,\n",
    "            start_lr=start_lr,\n",
    "            ref_lr=lr,\n",
    "            final_lr=final_lr,\n",
    "            iterations_per_epoch=ipe,\n",
    "            warmup=warmup,\n",
    "            num_epochs=args[\"num_epochs\"],\n",
    "            ipe_scale=ipe_scale,\n",
    "            use_bfloat16=use_bfloat16)\n",
    "\n",
    "    ema = [0.996, 1.0]\n",
    "    ipe = len(train_loader)\n",
    "    ipe_scale = 1.0\n",
    "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*args[\"num_epochs\"]*ipe_scale)\n",
    "                            for i in range(int(ipe*args[\"num_epochs\"]*ipe_scale)+1))\n",
    "    \n",
    "    if load_model:\n",
    "        encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
    "            device=device,\n",
    "            r_path=path,\n",
    "            encoder=encoder,\n",
    "            predictor=predictor,\n",
    "            target_encoder=target_encoder,\n",
    "            opt=optimizer,\n",
    "            scaler=scaler)\n",
    "        for _ in range(start_epoch*ipe):\n",
    "            scheduler.step()\n",
    "            wd_scheduler.step()\n",
    "\n",
    "            next(momentum_scheduler)\n",
    "    ## just get the attention maps from the context_encoder (input whole image)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(val_loader):\n",
    "            data = data[:, 0, :, :].squeeze(1)\n",
    "            B, N, D = data.size()\n",
    "            data = data.view(B, args[\"rows\"], args[\"cols\"], D).to(device)\n",
    "            imgs, masks_enc, masks_pred = generateMasks(data)\n",
    "            imgs = imgs.permute(0, 3, 1, 2)  # [B, D, R, C]\n",
    "            masks_pred = masks_pred.flatten(2)  # [B, 4, R*C]\n",
    "            masks_enc = masks_enc.flatten(1).unsqueeze(1)  # [B, 1, R*C]\n",
    "            \n",
    "            \"\"\" see whole image as context \"\"\"\n",
    "            masks_enc=torch.ones_like(masks_enc)\n",
    "            # masks_enc=torch.zeros_like(masks_enc)\n",
    "            # masks_enc[:, :, 0:2] = 1\n",
    "            def forward_context():    ## mask context tokens before encoding\n",
    "                z, attn_list = encoder(imgs, masks=masks_enc)  ## VisionTransformer\n",
    "                return z, attn_list\n",
    "            def forward_target():\n",
    "                z, attn_list = target_encoder(imgs, masks=None)  ## VisionTransformer\n",
    "                return z, attn_list\n",
    "            zt, _ = forward_target()  # Target encoder forward pass\n",
    "            zp = predictor(zt, masks_enc, masks_pred)   ## VisionTransformerPredictor\n",
    "            zc, attn_list_1 = forward_context()  # Context encoder forward pass\n",
    "            \n",
    "\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Generate the same 200x200 attention matrix\n",
    "    # np.random.seed(42)\n",
    "    attn1 = attn_list_1[l][0].squeeze(0).softmax(-1).cpu().numpy()[:,:]\n",
    "    print(zc.shape)\n",
    "    zc = zc[0].squeeze(0).cpu().numpy()\n",
    "    # Create a static heatmap using Matplotlib\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(zc, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    plt.title('Static Attention Matrix Heatmap')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "\n",
    "    zt = zt[0].squeeze(0).cpu().numpy()\n",
    "    # Create a static heatmap using Matplotlib\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(zt, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    plt.title('Static Attention Matrix Heatmap')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "\n",
    "    zp = zp[0].squeeze(0).cpu().numpy()\n",
    "    # Create a static heatmap using Matplotlib\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(zp, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    plt.title('Static Attention Matrix Heatmap')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(10, 501, 10):\n",
    "\n",
    "    load_path = fr\"D:\\omer\\ST-SSL\\logs\\singleBLK_try2\\jepa-ep{i}.pth.tar\"\n",
    "    plot_adj(load_path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1.shape\n",
    "import numpy as np\n",
    "np.savez(r'D:\\omer\\ST-SSL\\data\\NYCTaxi\\pretrained_adj_mx.npz', adj_mx=attn1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_list1 = [attn_list_1[i][0].squeeze(0).detach().cpu().softmax(-1).numpy() for i in range(6)]\n",
    "\n",
    "attn_array = np.array(attn_list1)\n",
    "average_attention = np.mean(attn_array, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Average Matrix:\", average_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn1 = attn_list[1][0].squeeze(0)\n",
    "# avg_attn = attn1\n",
    "# for i in range(1, len(attn_list)): avg_attn+=attn_list[i][0].squeeze(0)\n",
    "# avg_attn = avg_attn/len(attn_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i-jepaVENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
