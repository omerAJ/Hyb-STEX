2024-01-25 21:13:33: Experiment log path in: D:\omer\ST-SSL\experiments\Careem\20240125-211333
2024-01-25 21:13:33: Experiment configs are: Namespace(seed=31, device='cuda', mode='train', best_path='None', debug=False, data_dir='data', dataset='Careem', input_length=19, batch_size=32, test_batch_size=32, graph_file='data/Careem/adj_mx.npz', d_input=1, d_output=1, d_model=64, dropout=0.2, percent=0.1, shm_temp=0.5, nmb_prototype=2, yita=1, epochs=1, lr_init=0.001, early_stop=True, early_stop_patience=150, grad_norm=True, max_grad_norm=5, use_dwa=True, temp=4, num_nodes=285, log_dir='D:\\omer\\ST-SSL\\experiments\\Careem\\20240125-211333')
2024-01-25 21:13:37: Traceback (most recent call last):
  File "D:\omer\ST-SSL\main.py", line 59, in model_supervisor
    results = trainer.train() # best_eval_loss, best_epoch
              ^^^^^^^^^^^^^^^
  File "D:\omer\ST-SSL\model\trainer.py", line 109, in train
    train_epoch_loss, loss_t = self.train_epoch(epoch, loss_weights)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\omer\ST-SSL\model\trainer.py", line 58, in train_epoch
    loss, sep_loss = self.model.loss(repr1, repr2, target, self.scaler, loss_weights)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\omer\ST-SSL\model\models.py", line 62, in loss
    l1 = self.pred_loss(z1, z2, y_true, scaler)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\omer\ST-SSL\model\models.py", line 79, in pred_loss
    loss = self.args.yita * self.mae(y_pred[..., 0], y_true[..., 0])
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\omer\ST-SSL\lib\utils.py", line 11, in loss
    mae = mae_torch(pred=preds, true=labels, mask_value=mask_value)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\omer\ST-SSL\lib\metrics.py", line 7, in mae_torch
    pred = torch.masked_select(pred, mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


